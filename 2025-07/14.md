## 2025-07-14

### 📚 공부한 내용

최근 쿠폰 발급 서비스의 **부하 테스트**를 진행했다.  
주요 목표는:

- 쿠폰 오픈 시 특정 시간대에 몰리는 트래픽 대응
- 짧은 시간 안에 쿠폰 재고 소진 시 동시성 처리 및 락 관리
- RPS가 선형적으로 증가(Linear Scalability)하는지 검증
- 향후 스케일아웃으로 안정적 대응 가능 여부 판단

---

### 💡 주요 개념들

#### 사용한 기술 스택 & 툴

- **Locust** → 부하 테스트 툴
- **Spring Boot** → 백엔드 서버
- **Redis + Redisson** → 분산 락 처리
- **배치 프로세스** → DB 동기화

---

#### Locust로 부하 테스트

- 동시 접속 시나리오 구현
- `wait_time = 0` → 동시 사용자 요청 시뮬레이션

```python
from locust import HttpUser, task, constant

class CouponUser(HttpUser):
    wait_time = constant(0)

    @task
    def issue_coupon(self):
        coupon_id = 1
        with self.client.post(
            f"/api/v1/user-coupons/{coupon_id}/issue",
            catch_response=True
        ) as resp:
            if resp.status_code == 200:
                print("쿠폰 발급 성공")
            elif resp.status_code == 204:
                print("쿠폰 재고 소진")
            else:
                resp.failure(f"발급 실패: {resp.text}")
```

---

#### Redis 설정 (Redisson)

- Redis는 싱글 쓰레드 기반 → 커넥션 풀 사이즈 작게 설정
- Redisson은 TTL, 재시도, Pub/Sub 기반 분산 락을 지원

```java
@Bean
public RedissonClient redissonClient() {
    Config config = new Config();
    config.setCodec(new StringCodec());
    config.useSingleServer()
        .setAddress("redis://127.0.0.1:6379")
        .setConnectionPoolSize(1)
        .setConnectionMinimumIdleSize(1)
        .setTimeout(3000);
    return Redisson.create(config);
}
```

---

#### 락 기반 쿠폰 재고 감소 로직

```java
public int decreaseCouponStock(Long couponId) {
    String lockKey = "lock:coupon:" + couponId;
    String dataKey = "coupon:" + couponId;

    return couponRedisRepository.executeWithLock(
            lockKey,
            3, // waitSeconds
            3, // leaseSeconds
            () -> couponRedisRepository.decreaseCouponStock(dataKey)
                    .orElseThrow(() -> new UserCouponException("SOLD_OUT"))
    );
}
```

- 락 획득 → 재고 감소 로직 실행
- 재고 소진 시 예외 처리

---

#### 부하 테스트 결과 요약

- **10명 동접**
  - RPS 약 285
  - 대부분 50th percentile에서 매우 빠름
  - 일부 200ms 응답 시간 관찰 → 초기 커넥션 핸드쉐이크 영향

- **30명 동접**
  - 평균 응답속도: 114ms
  - 상위 5%: 620ms
  - 상위 1%: 1.2초
  - 최악: 2초
  - 락 경합으로 실패 12건 (0.1%)

- **200명 동접**
  - 응답시간 소폭 증가
  - RPS 선형적으로 증가 → Linear Scalability 검증
  - 일부 1.1~2초, 최대 3.7초 소요
  - 락 처리 실패 126건 (0.61%)

---

#### 병목 원인 & 개선점

- Redis 단일 커넥션의 처리량 한계
- Redis Lock 경합으로 인한 지연
- 배치 동기화 방식 → 실시간성이 떨어지고, SCAN 명령 O(n) 부담

---

#### 개선 방향

- 배치 동기화 → RabbitMQ 이벤트 기반 전환 검토
- Redis 락 범위 개선 → 분할 키, TTL 최적화
- 모니터링 강화 → 병목 구간 정확히 파악
- Redis 클러스터 확장 고려

---

#### 배치 동기화 현황

- 쿠폰 오픈 배치 → 쿠폰 발급 시작 전 Redis preload
- 쿠폰 클로즈 배치 → 발급 종료 시점 Redis → DB 동기화
- 쿠폰 재고 동기화 배치 → 일정 주기로 Redis 재고를 DB와 sync

현재 배치 방식의 한계:

- 실시간 반영이 어려움
- 배치 주기 동안 stale data 노출
- Redis SCAN → O(n) 연산으로 부하 우려

---

### 🔗 참고

- [Locust 공식 문서](https://locust.io/)
- [Redisson 공식 문서](https://github.com/redisson/redisson)
- [블로그 정리 글](https://new-think-think.tistory.com/49)

---

```

